The dataset provided by TREC committee consists of approximately 4.5 TB of compressed and thrift serialized (1.046 billion documents) unstructured data obtained by crawling the web. It contains news articles, blog entries, social data, including twitter, yahoo answers etc. and research papers from arxiv. On an average, there are around $10^{5}$ articles per hour. In addition to the body of the pages, the dataset consists of NER and coreference information obtained using LingPipe (although the NER information is not really good).

In addition, we also used the Google Cross Lingual Dictionary (GCLD)~\cite{gcld} dataset. It contains a mapping from common anchor texts to wikipedia concepts, spanning 7,560,141 concepts and 175,100,788 unique text strings.

